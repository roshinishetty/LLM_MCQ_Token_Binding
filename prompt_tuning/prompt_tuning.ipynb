{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807ddf36-b84f-4d69-995c-6779bdc55a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "!pip install --pre -U xformers -q\n",
    "\n",
    "#Import all Libraries.\n",
    "!pip install -r \"requirements.txt\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4789f311-e08e-402a-844e-825f0d9ab626",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6193ff30-85b8-4946-b501-720a54bf0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import default_data_collator, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_model, TaskType, PeftType, PrefixTuningConfig, PromptTuningConfig, PromptTuningInit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dd1201c-d50b-4b2f-8cdf-1e0249e1a3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/jupyter/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Please use your huggingface credentials\n",
    "!huggingface-cli login --token \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9712a175-d40c-4d15-aa73-d68b62695f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00413e99-cd53-400c-83a3-36224a902466",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Unused kwargs: ['quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_name_or_path = \"unsloth/llama-2-7b-bnb-4bit\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side='right', padding=True, \n",
    "                                         truncation=True, max_length=max_seq_length, low_cpu_mem_usage=True, device_map=\"auto\")\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a223d52-9dcf-46fb-b27b-673b62f20ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_config = {\n",
    "    'lr': 1e-4,\n",
    "    'nepochs': 4,\n",
    "    'batch_size':4,\n",
    "    'wd': 1e-7,\n",
    "    'eps': 0.1,\n",
    "    'warmup_steps': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ad134a8-5723-4393-82c8-224ce66bc8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 40,960 || all params: 6,738,456,576 || trainable%: 0.0006078543289257816\n"
     ]
    }
   ],
   "source": [
    "# Prompt tuning\n",
    "num_virtual_tokens=10\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=num_virtual_tokens,\n",
    "    prompt_tuning_init_text=\"Focus on the answer not the option id or position of correct answer.\",\n",
    "    tokenizer_name_or_path=model_name_or_path,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters() #Ensure only prompt tuning params are trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc37ab-47a9-4993-ba33-779ff2f9012c",
   "metadata": {},
   "source": [
    "Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cc374ec-50dc-4b9d-be6a-ed9270eebd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"mmlu_01/permuted_trainset_16k_new.csv\"\n",
    "eval_file = \"mmlu_01/permuted_valset_16k_new.csv\"\n",
    "test_file = \"mmlu_01/varying_option/permuted_testset_16k_new.csv\"\n",
    "\n",
    "train_dataset =  load_dataset('csv', data_files=train_file, split='train')\n",
    "eval_dataset = load_dataset('csv', data_files=eval_file, split='train')\n",
    "test_dataset = load_dataset('csv', data_files=test_file, split='train')\n",
    "\n",
    "# train_dataset = pick_samples(train_dataset, 300) # Uncomment for taking smaller training set to test the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e7706-b7e6-4950-9fb7-169ad0a9470c",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ba889f9-ee42-4a98-8ef9-b5dbababd404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and lr scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr=hyperparam_config['lr'], \n",
    "                              weight_decay=hyperparam_config['wd'], \n",
    "                              eps=hyperparam_config['eps'])\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=hyperparam_config['warmup_steps'],\n",
    "    num_training_steps=(len(train_dataset) * hyperparam_config['nepochs']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acedd31d-3f85-4052-b9f6-4acebfc597c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79169195-17f1-400b-9379-dda1648fbf1c",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bde7809b-602c-484f-8747-de9f0946ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"train_logs/llama-2-7b\",\n",
    "    learning_rate=hyperparam_config['lr'],\n",
    "    per_device_train_batch_size=hyperparam_config['batch_size'],\n",
    "    per_device_eval_batch_size=hyperparam_config['batch_size'],\n",
    "    num_train_epochs=hyperparam_config['nepochs'],\n",
    "    weight_decay=hyperparam_config['wd'],\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,\n",
    "    group_by_length=True,\n",
    "    dataloader_pin_memory=True,                           \n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_prefetch_factor=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "447b9e3b-2719-40da-aefa-56d5069b32e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1861d901c704f02a69b2f90c71bf008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dace37b4254f8cb1fa324c69d2a78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1279448"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "context_prompt = '''### Instruction: Dig into your knowledge and come up with an answer from the options A/B/C/D given below. \n",
    "Then, choose the option that best completes the sentence regardless of its position. \\n\\n'''\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    question = example['question'][:-10]\n",
    "    text = f\"{context_prompt} ### {question}\\n\\n  ###Answer: {example['gold_answer']}\"\n",
    "    return {'prompt': text}\n",
    "\n",
    "response_template = \"###Answer:\"\n",
    "\n",
    "train_dataset = train_dataset.map(formatting_prompts_func)\n",
    "train_dataset.to_csv('train_dataset.csv')\n",
    "eval_dataset = eval_dataset.map(formatting_prompts_func)\n",
    "eval_dataset.to_csv('eval_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a16402-ed38-41b6-81ec-9602bd076d8e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340b5933f5554721b289814803302927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84681ead55d64e1ca71746cf228c8ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1018 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7122' max='16000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7122/16000 2:13:35 < 2:46:34, 0.89 it/s, Epoch 1.78/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.159600</td>\n",
       "      <td>1.314077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    "    dataset_text_field=\"prompt\", \n",
    "    max_seq_length=max_seq_length, \n",
    "    data_collator=collator, \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567c0aaf-7a48-423f-8e72-0f3794aa0cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model\n",
    "model.push_to_hub(\"llama-2-7b-prompt-tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635705e1-27ba-4e2d-84ad-bb2f224283c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"out_dir/llama-2-7b-prompt-tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d1bd0-a7f9-48c2-b740-8aaabec65b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate answers to questions\n",
    "def generate_answer(question, vocab_id_A, vocab_id_B, vocab_id_C, vocab_id_D):\n",
    "  max_new_tokens = 1\n",
    "  question = question[:-10]\n",
    "  question = f\"{context_prompt} ### {question}\\n\\n ### Answer: \"\n",
    "  input_ids = tokenizer.encode(question, return_tensors=\"pt\").cuda()\n",
    "  output_ids = model.generate(input_ids, max_new_tokens=max_new_tokens,return_dict=True,return_dict_in_generate=True,output_scores=True,do_sample=False)\n",
    "  probs = output_ids.scores[0].softmax(-1).squeeze().cuda()\n",
    "\n",
    "  #Scores is a tuple : (tensor of scores, empty) and tensor is of shape (1,vocab size of model)\n",
    "\n",
    "  #Finding Probability of generating current predicted token output\n",
    "  gen = tokenizer.decode(output_ids.sequences[0], skip_special_tokens=True)\n",
    "  answer = gen[-max_new_tokens:]\n",
    "\n",
    "  #Finding Probability of generating other tokens as answers ('A','B','C','D')\n",
    "  #Assert that probability of prediction of the correct option is the same as above.\n",
    "  vocab_id_A_probs = probs[vocab_id_A].item()\n",
    "  vocab_id_B_probs = probs[vocab_id_B].item()\n",
    "  vocab_id_C_probs = probs[vocab_id_C].item()\n",
    "  vocab_id_D_probs = probs[vocab_id_D].item()\n",
    "  total = vocab_id_A_probs + vocab_id_B_probs + vocab_id_C_probs + vocab_id_D_probs\n",
    "  vocab_id_A_probs = vocab_id_A_probs/total\n",
    "  vocab_id_B_probs = vocab_id_B_probs/total\n",
    "  vocab_id_C_probs = vocab_id_C_probs/total\n",
    "  vocab_id_D_probs = vocab_id_D_probs/total\n",
    "\n",
    "  return answer, vocab_id_A_probs, vocab_id_B_probs, vocab_id_C_probs, vocab_id_D_probs\n",
    "\n",
    "# Function to process CSV file containing questions\n",
    "\n",
    "def process_csv(input_file, output_file):\n",
    "  index = 0\n",
    "  with open(input_csv_file, \"r\") as input_file, open(output_csv_file, \"w\", newline=\"\") as output_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    writer = csv.writer(output_file)\n",
    "\n",
    "    # Read and write headers\n",
    "    header = next(reader)\n",
    "    writer.writerow(header + [\"Predicted_token_ID\", \"Normalized_A_probs\", \"Normalized_B_probs\", \"Normalized_C_probs\", \"Normalized_D_probs\"])\n",
    "\n",
    "    # Process each row in the CSV file\n",
    "    for row in reader:\n",
    "        question = row[0]\n",
    "        if question == '':\n",
    "          break\n",
    "\n",
    "        #get vocab keys for each of the option ID's to index the scores tensor based on these vocab id's\n",
    "        out = tokenizer.get_vocab()\n",
    "        vocab_id_A = out['A']\n",
    "        vocab_id_B = out['B']\n",
    "        vocab_id_C = out['C']\n",
    "        vocab_id_D = out['D']\n",
    "\n",
    "        answer, A_norm_prob, B_norm_prob, C_norm_prob, D_norm_prob = generate_answer(question,\n",
    "                                                                                     vocab_id_A, vocab_id_B, vocab_id_C, vocab_id_D)\n",
    "        print(f'{index}: {answer}, {A_norm_prob}, {B_norm_prob}, {C_norm_prob}, {D_norm_prob}')\n",
    "        writer.writerow(row + [answer, A_norm_prob, B_norm_prob, C_norm_prob, D_norm_prob])\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4277d796-e938-489a-8a99-cc400aeceb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tables = [\n",
    "  \"permuted_testset_16k_new\"\n",
    "  \"professional_law\",\n",
    "  \"prehistory\",\n",
    "  \"philosophy\",\n",
    "  \"high_school_mathematics\",\n",
    "  \"conceptual_physics\",\n",
    "  \"college_medicine\",\n",
    "  \"abstract_algebra\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51149924-e031-40cc-9195-55aa45269866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Input and output file paths\n",
    "model_name = \"llama-2-7b\"\n",
    "mmlu_01 = \"mmlu_01/\"\n",
    "mmlu_02 = \"prompt_tuned/\"\n",
    "\n",
    "for is_varying_option in [True, False]:\n",
    "    sub_folder = \"varying_option\" if is_varying_option else \"varying_position\"\n",
    "    for file_name in data_tables:\n",
    "      input_csv_file = mmlu_01 + sub_folder + \"/\" + file_name + \".csv\"\n",
    "      output_csv_file = mmlu_02 + model_name + \"/\" + sub_folder + \"/\" + file_name + \".csv\"\n",
    "\n",
    "      # Process CSV file containing questions and generate answers\n",
    "      process_csv(input_csv_file, output_csv_file)\n",
    "\n",
    "      print(\"Answers generated and saved to:\", output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50015ec-201a-46d6-b93d-14d5e81734a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
